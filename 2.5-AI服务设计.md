# 2.5 AI 服务设计

## 1. 核心功能：上下文智能探针

### 1.1 功能定位

上下文智能探针 (Context-Aware Intelligent Probe) 是 InsightReader 的核心AI功能，它能够：

1. **理解上下文**: 结合选中文本和周围语境
2. **识别意图**: 判断用户想了解什么
3. **生成洞察**: 用通俗语言提供有价值的解释或分析
4. **流式输出**: 逐字返回，提供即时反馈

---

## 2. Prompt 工程

### 2.1 系统 Prompt 设计

#### **解释意图 (explain)**

```markdown
你是一位知识渊博的教育者，擅长用通俗易懂的语言解释复杂概念。

**你的回答应该：**
1. 简洁明了，控制在 200 字以内
2. 从基础开始，避免假设用户已有高深知识
3. 使用类比和例子帮助理解
4. 如果涉及专业术语，先解释术语本身
5. 用 Markdown 格式组织内容

**回答结构建议：**
- **定义**: 这是什么
- **背景**: 为什么重要/出现的原因
- **举例**: 生活中的类比或实例
```

#### **分析意图 (analyze)**

```markdown
你是一位批判性思维专家，擅长分析文本的逻辑结构和论证手法。

**你的回答应该：**
1. 指出论证类型（演绎、归纳、类比、权威等）
2. 分析前提和结论的关系
3. 评估论证的强度
4. 指出潜在的逻辑漏洞或偏见
5. 保持客观，不带入个人立场

**回答结构建议：**
- **论证类型**: 识别使用的推理方式
- **论证结构**: 前提 → 推理 → 结论
- **强弱评估**: 论证是否有说服力
- **潜在问题**: 可能的漏洞或隐含假设
```

#### **反驳意图 (counter)** [V2.0]

```markdown
你是一位思维开阔的分析师，能从多个角度看待问题。

**你的回答应该：**
1. 提供至少一个有代表性的反方观点
2. 说明这些观点的依据和合理性
3. 指出争议的焦点
4. 保持客观中立，不偏袒任何一方

**回答结构建议：**
- **反方观点**: 对立的看法是什么
- **依据**: 为什么有人持这种观点
- **争议焦点**: 分歧的根本原因
```

---

### 2.2 用户 Prompt 模板

#### **模板 1: 解释概念**

```markdown
请解释以下文本的含义：

**选中的文本**：{selected_text}

**上下文**：
{context}

请简要解释：
1. 这个概念/术语的含义
2. 相关的背景知识
3. 为什么它在这里被提及

请用通俗易懂的语言，控制在 200 字以内。
```

**实际示例**:
```markdown
请解释以下文本的含义：

**选中的文本**：康德的绝对命令

**上下文**：
在伦理学中，康德的绝对命令是一个核心概念。它要求我们的行为准则能够成为普遍法则。这种道义论的观点与功利主义形成鲜明对比。

请简要解释：
1. 这个概念/术语的含义
2. 相关的背景知识
3. 为什么它在这里被提及

请用通俗易懂的语言，控制在 200 字以内。
```

**期望输出**:
```markdown
**康德的绝对命令** (Categorical Imperative) 是德国哲学家康德提出的核心伦理原则。

**含义**：简单来说，就是"你希望自己的行为规则能被所有人遵守吗？"比如，如果你撒谎，你能接受"人人都撒谎"成为社会规则吗？显然不行。

**背景**：康德认为，道德不应该基于后果（功利），而应该基于义务本身。一个行为的对错，不看它带来什么好处，而看它是否符合普遍法则。

**在此处的意义**：文中提到它与功利主义对比，正是因为功利主义注重"结果"（最大幸福），而康德注重"动机"（遵守普遍法则）。两者是伦理学的两大流派。
```

---

#### **模板 2: 分析论证**

```markdown
请分析以下文本的论证逻辑：

**选中的文本**：{selected_text}

**上下文**：
{context}

请分析：
1. 作者使用了什么类型的论证
2. 主要论据和结论是什么
3. 论证是否有说服力
4. 可能存在的漏洞或弱点

请简洁回答，控制在 250 字以内。
```

**实际示例**:
```markdown
请分析以下文本的论证逻辑：

**选中的文本**：深度学习的成功很大程度上归功于数据的规模。更多的数据意味着模型能学到更丰富的模式，从而提升泛化能力。

**上下文**：
近年来，深度学习在图像识别、自然语言处理等领域取得了巨大成功。深度学习的成功很大程度上归功于数据的规模。更多的数据意味着模型能学到更丰富的模式，从而提升泛化能力。然而，数据并非万能，模型架构和训练技巧同样重要。

请分析：
1. 作者使用了什么类型的论证
2. 主要论据和结论是什么
3. 论证是否有说服力
4. 可能存在的漏洞或弱点

请简洁回答，控制在 250 字以内。
```

**期望输出**:
```markdown
**论证类型**：因果论证

**论证结构**：
- **前提**：深度学习需要学习数据中的模式
- **推理**：更多数据 → 更丰富的模式 → 更好的泛化
- **结论**：深度学习的成功归功于数据规模

**说服力评估**：
- **优点**：逻辑链条清晰，符合直觉（大数据时代的共识）
- **支持证据**：ImageNet、GPT 等都是大规模数据训练的成功案例

**潜在问题**：
1. **简化因果关系**：将"成功"主要归因于数据，忽略了其他关键因素（如架构创新、优化算法）
2. **缺乏反例讨论**：未提及小数据集上的成功案例（如迁移学习、少样本学习）
3. **"很大程度"模糊**：没有量化数据贡献的具体比例

**改进建议**：补充说明数据、算法、算力三者的协同作用。
```

---

### 2.3 上下文提取策略

```python
def extract_context(
    full_text: str,
    selected_text: str,
    context_window: int = 200
) -> str:
    """
    提取选中文本周围的上下文

    Args:
        full_text: 完整文章内容
        selected_text: 用户选中的文本
        context_window: 前后各提取的字符数

    Returns:
        包含选中文本的上下文片段
    """
    # 找到选中文本的位置
    start_index = full_text.find(selected_text)

    if start_index == -1:
        # 如果找不到，尝试模糊匹配
        start_index = fuzzy_find(full_text, selected_text)

    if start_index == -1:
        return selected_text  # 实在找不到，只返回选中文本

    # 计算上下文边界
    context_start = max(0, start_index - context_window)
    context_end = min(len(full_text), start_index + len(selected_text) + context_window)

    # 提取上下文
    context = full_text[context_start:context_end]

    # 智能截断：避免截断句子
    context = smart_truncate(context, start_index - context_start, len(selected_text))

    return context

def smart_truncate(text: str, selected_start: int, selected_length: int) -> str:
    """
    智能截断，避免破坏句子完整性
    """
    # 找到选中文本前后最近的句子边界
    before = text[:selected_start]
    selected = text[selected_start:selected_start + selected_length]
    after = text[selected_start + selected_length:]

    # 向前找句子开始
    sentence_start_markers = ['。', '！', '？', '.', '!', '?', '\n\n']
    for marker in sentence_start_markers:
        pos = before.rfind(marker)
        if pos != -1:
            before = before[pos + 1:].lstrip()
            break

    # 向后找句子结束
    for marker in sentence_start_markers:
        pos = after.find(marker)
        if pos != -1:
            after = after[:pos + 1].rstrip()
            break

    return before + selected + after
```

---

## 3. 意图识别

### 3.1 简单规则（MVP）

```python
def detect_intent(selected_text: str) -> str:
    """
    基于简单规则识别用户意图

    Returns:
        "explain" | "analyze" | "counter"
    """
    text_len = len(selected_text)

    # 规则 1: 长度判断
    if text_len < 5:
        return "explain"  # 短文本倾向于是术语

    if text_len > 50:
        return "analyze"  # 长文本倾向于是论述

    # 规则 2: 关键词判断
    causal_keywords = ['因为', '所以', '由于', '导致', '原因', '结果']
    if any(keyword in selected_text for keyword in causal_keywords):
        return "analyze"  # 包含因果关系的文本适合分析

    # 规则 3: 符号判断
    if '？' in selected_text or '?' in selected_text:
        return "analyze"  # 问句适合分析

    # 默认返回解释
    return "explain"
```

### 3.2 ML 模型（V2.0）

```python
from transformers import pipeline

class IntentClassifier:
    def __init__(self):
        self.classifier = pipeline(
            "text-classification",
            model="intent-classification-model"
        )

    def predict(self, selected_text: str, context: str) -> str:
        """
        使用机器学习模型预测意图

        Args:
            selected_text: 选中的文本
            context: 上下文

        Returns:
            "explain" | "analyze" | "counter"
        """
        input_text = f"选中: {selected_text}\n上下文: {context}"
        result = self.classifier(input_text)

        intent_map = {
            "LABEL_0": "explain",
            "LABEL_1": "analyze",
            "LABEL_2": "counter"
        }

        return intent_map.get(result[0]['label'], "explain")
```

---

## 4. 模型选择策略

### 4.1 基于成本的智能选择

```python
class ModelSelector:
    """智能选择 AI 模型"""

    MODELS = {
        "haiku": {
            "name": "claude-3-haiku-20240307",
            "cost_per_1k_tokens": 0.00025,  # 输入
            "suitable_for": ["explain_simple", "short_text"]
        },
        "sonnet": {
            "name": "claude-3-5-sonnet-20241022",
            "cost_per_1k_tokens": 0.003,  # 输入
            "suitable_for": ["analyze", "complex_explain", "counter"]
        }
    }

    @classmethod
    def select(cls, intent: str, text_length: int) -> str:
        """
        选择合适的模型

        Args:
            intent: 用户意图
            text_length: 选中文本长度

        Returns:
            模型名称
        """
        # 简单解释 + 短文本 → Haiku
        if intent == "explain" and text_length < 10:
            return cls.MODELS["haiku"]["name"]

        # 复杂分析 → Sonnet
        if intent in ["analyze", "counter"]:
            return cls.MODELS["sonnet"]["name"]

        # 长文本 → Sonnet
        if text_length > 50:
            return cls.MODELS["sonnet"]["name"]

        # 默认 Sonnet（质量优先）
        return cls.MODELS["sonnet"]["name"]
```

---

## 5. 流式响应优化

### 5.1 分块策略

```python
async def stream_with_smart_chunking(text_stream):
    """
    智能分块：按词组而非单字返回

    好处：
    1. 减少网络请求次数
    2. 更符合阅读习惯
    3. 降低前端渲染压力
    """
    buffer = ""

    async for chunk in text_stream:
        buffer += chunk

        # 遇到标点或空格时发送
        if chunk in [' ', '，', '。', '、', '；', '\n']:
            yield buffer
            buffer = ""

    # 发送剩余内容
    if buffer:
        yield buffer
```

### 5.2 错误恢复

```python
async def generate_with_retry(
    ai_service: AIService,
    request: InsightRequest,
    max_retries: int = 3
):
    """
    带重试的生成逻辑
    """
    for attempt in range(max_retries):
        try:
            async for chunk in ai_service.generate_insight_stream(request):
                yield chunk
            return  # 成功，退出

        except asyncio.TimeoutError:
            if attempt < max_retries - 1:
                yield {"type": "error", "message": "超时，正在重试..."}
                await asyncio.sleep(1)
            else:
                yield {"type": "error", "message": "生成失败，请稍后重试"}

        except Exception as e:
            yield {"type": "error", "message": f"错误: {str(e)}"}
            return
```

---

## 6. Prompt 优化技巧

### 6.1 Few-Shot Learning

```markdown
你是一位知识渊博的教育者，擅长用通俗易懂的语言解释复杂概念。

**示例1：**
选中：量子纠缠
回答：量子纠缠是量子力学中的一种现象，简单来说就是两个粒子之间存在神秘的"心灵感应"。当你测量其中一个粒子的状态时，另一个粒子会立即"知道"并改变自己的状态，无论它们相距多远...

**示例2：**
选中：边际效用递减
回答：边际效用递减是经济学原理，意思是"同一种东西，第一次获得时最开心，之后逐渐变得不那么开心"。比如你很饿时吃第一个包子特别满足（效用高），但吃到第五个时就没那么香了（效用递减）...

**现在，请解释：**
选中的文本：{selected_text}
上下文：{context}
```

### 6.2 Chain-of-Thought

```markdown
请分析以下文本的论证逻辑：

选中的文本：{selected_text}
上下文：{context}

**请按以下步骤思考：**
1. 首先，识别作者的主要结论是什么
2. 然后，找出支持这个结论的论据
3. 接着，判断这些论据如何推导出结论
4. 最后，评估这个推理过程是否严密

请逐步呈现你的分析过程。
```

---

## 7. 质量保证

### 7.1 输出验证

```python
def validate_insight(insight: str) -> tuple[bool, str]:
    """
    验证 AI 输出质量

    Returns:
        (是否通过, 错误信息)
    """
    # 检查长度
    if len(insight) < 20:
        return False, "回答过短"

    if len(insight) > 1000:
        return False, "回答过长"

    # 检查是否拒绝回答
    refusal_phrases = ["我无法", "我不能", "抱歉", "无法回答"]
    if any(phrase in insight for phrase in refusal_phrases):
        return False, "AI 拒绝回答"

    # 检查是否包含有用信息
    if insight.count('\n') < 2:
        return False, "结构过于简单"

    return True, ""
```

### 7.2 A/B 测试框架 (V2.0)

```python
class PromptABTest:
    """Prompt A/B 测试"""

    def __init__(self):
        self.variants = {
            "A": PromptVariantA(),
            "B": PromptVariantB()
        }
        self.metrics = defaultdict(list)

    async def generate(self, request: InsightRequest):
        """随机选择一个 Prompt 变体"""
        variant = random.choice(["A", "B"])

        start_time = time.time()
        result = await self.variants[variant].generate(request)
        duration = time.time() - start_time

        # 记录指标
        self.metrics[variant].append({
            "duration": duration,
            "length": len(result),
            "timestamp": time.time()
        })

        return result, variant

    def get_winner(self):
        """分析哪个变体更好"""
        # 基于用户反馈、生成速度、输出质量等指标
        ...
```

---

## 8. 成本控制

### 8.1 缓存策略

```python
from functools import lru_cache
import hashlib

class InsightCache:
    """洞察缓存"""

    def __init__(self):
        self.cache = {}

    def get_cache_key(self, selected_text: str, context: str, intent: str) -> str:
        """生成缓存键"""
        content = f"{selected_text}|{context}|{intent}"
        return hashlib.md5(content.encode()).hexdigest()

    async def get_or_generate(
        self,
        selected_text: str,
        context: str,
        intent: str,
        generator: callable
    ):
        """获取缓存或生成新洞察"""
        cache_key = self.get_cache_key(selected_text, context, intent)

        if cache_key in self.cache:
            # 缓存命中
            return self.cache[cache_key], True

        # 生成新洞察
        result = await generator(selected_text, context, intent)
        self.cache[cache_key] = result

        return result, False
```

### 8.2 成本监控

```python
class CostMonitor:
    """成本监控"""

    def __init__(self):
        self.total_tokens = 0
        self.total_cost = 0

    def record(self, model: str, input_tokens: int, output_tokens: int):
        """记录一次调用的成本"""
        cost = calculate_cost(model, input_tokens, output_tokens)
        self.total_tokens += input_tokens + output_tokens
        self.total_cost += cost

        # 记录到日志
        logger.info(f"AI调用: {model}, tokens: {input_tokens}+{output_tokens}, cost: ${cost:.4f}")

    def get_stats(self) -> dict:
        """获取统计数据"""
        return {
            "total_tokens": self.total_tokens,
            "total_cost": self.total_cost,
            "avg_cost_per_request": self.total_cost / self.request_count if self.request_count > 0 else 0
        }
```

---

**文档版本**: 2.0（基于最新产品蓝图）
**最后更新**: 2025-01-19
